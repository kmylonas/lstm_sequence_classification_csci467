{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "sRggT_kK32id",
        "outputId": "b77343fb-314a-4b89-b072-55d24b0a4cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU activated\n",
            "Creating Vocabulary from dataset...\n",
            "Reading GloVe...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1193513/1193514 [01:07<00:00, 17635.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting weights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing train data: 100%|██████████| 291766/291766 [00:09<00:00, 31501.06it/s]\n",
            "Preprocessing dev data: 100%|██████████| 83778/83778 [00:01<00:00, 44566.74it/s]\n",
            "Preprocessing test data: 100%|██████████| 41265/41265 [00:01<00:00, 31542.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Epoch 0 : train_acc=0.34987, dev_acc=0.37601, train_f1=0.14295, dev_f1=0.16400 dev_loss=1.55977\n",
            "Epoch 1 : train_acc=0.39537, dev_acc=0.39506, train_f1=0.19138, dev_f1=0.17961 dev_loss=1.53259\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e6b167cb29a2>\u001b[0m in \u001b[0;36m<cell line: 426>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e6b167cb29a2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     train_accuracies, dev_accuracies = train(model, train_set, dev_set, lr=1e-1,\n\u001b[0m\u001b[1;32m    404\u001b[0m           batch_size=batch_size, num_epochs=epochs, class_weights=class_weights)\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e6b167cb29a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_set, dev_set, class_weights, lr, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# Compute the loss of the model output compared to true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run backpropagation to compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Take a SGD step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Note that when we created the optimizer, we passed in model.parameters()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vectors\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "#sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)\n",
        "\n",
        "mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "\n",
        "X_train_filename = \"./drive/MyDrive/CSCI467Data/X_train.tsv\"\n",
        "X_dev_filename = \"./drive/MyDrive/CSCI467Data/X_dev.tsv\"\n",
        "X_test_filename = \"./drive/MyDrive/CSCI467Data/X_test.tsv\"\n",
        "y_train_filename = \"./drive/MyDrive/CSCI467Data/y_train.npy\"\n",
        "y_dev_filename = \"./drive/MyDrive/CSCI467Data/y_dev.npy\"\n",
        "y_test_filename = \"./drive/MyDrive/CSCI467Data/y_test.npy\"\n",
        "\n",
        "data_filename = \"./drive/MyDrive/CSCI467Data/data.jsonl\"\n",
        "# word_vectors_filename = \"./drive/MyDrive/CSCI467Data/glove.6B.50d.txt\"\n",
        "word_vectors_filename = \"./drive/MyDrive/CSCI467Data/glove.twitter.27B.100d.txt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embed_size = 100\n",
        "hidden_dim = 32\n",
        "n_layers = 1\n",
        "num_of_classes = 6\n",
        "dropout = 0.2\n",
        "batch_size = 2048\n",
        "epochs = 100\n",
        "freeze = False\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU activated\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU activated\")\n",
        "\n",
        "\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, X, y,seq_len, transform=None, target_transform=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.seq_len[idx]\n",
        "\n",
        "\n",
        "\n",
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, weights, hidden_size=50, n_layers=1, dropout=0, embed_size=50):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings=weights, freeze=freeze, padding_idx=0)\n",
        "        # self.rnn = nn.RNN(input_size = embed_size, hidden_size = hidden_dim, num_layers = n_layers, batch_first = True)\n",
        "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(hidden_dim, num_of_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "        embeddings = self.embedding_layer(x)\n",
        "        packed_embeddings = pack_padded_sequence(embeddings, seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "        # output, hidden = self.rnn(packed_embeddings, torch.randn(n_layers, len(x), hidden_dim))\n",
        "        packed_output,(hidden,cell) = self.lstm(packed_embeddings, (torch.randn(n_layers, len(x), hidden_dim).to(device), torch.randn(n_layers, len(x), hidden_dim).to(device)))\n",
        "        # return self.linear(hidden[-1])\n",
        "        a1 = F.relu(self.linear1(hidden[-1]))\n",
        "        d1 = self.dropout(a1)\n",
        "        output = self.linear2(d1)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_file(filename):\n",
        "    data=[]\n",
        "\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            words = line.strip().split('\\t')\n",
        "            data.append(words)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def get_vocabulary(data):\n",
        "    return list(set(word for words in data for word in words))\n",
        "\n",
        "def evaluate(model, dataset, name, class_weights, print_confusion_matrix = False):\n",
        "    \"\"\"Measure and print accuracy of a predictor on a dataset.\"\"\"\n",
        "    confusion_counts = Counter()\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    model.eval()  # Set model to \"eval mode\", e.g. turns dropout off if you have dropout layers.\n",
        "    with torch.no_grad():  # Don't allocate memory for storing gradients, more efficient when not training\n",
        "      for batch in DataLoader(dataset, batch_size=len(dataset), shuffle=True):\n",
        "        X, y, seq_len_batch = batch\n",
        "        X, y, seq_len_batch = X.to(device), y.to(device), seq_len_batch\n",
        "        logits = model(X, seq_len_batch)  # tensor of size (N, 10)\n",
        "        # Choose argmax for each row (i.e., collapse dimension 1, hence dim=1)\n",
        "        y_preds = torch.argmax(logits, dim=1)\n",
        "        loss = loss_func(logits, y)\n",
        "\n",
        "        acc = torch.mean((y_preds == y).float()).item()\n",
        "\n",
        "\n",
        "        f1s = f1_score(y.tolist(), y_preds.tolist(), average=None)\n",
        "        f1 = np.dot(class_weights, f1s)\n",
        "\n",
        "        for label, pred_label in zip(y.tolist(), y_preds.tolist()):\n",
        "            confusion_counts[(label, pred_label)] += 1\n",
        "\n",
        "    print(f'Accuracy on {name} data: {acc:.5f}')\n",
        "    print(f'Weighted F1 on {name} data: {f1.item():.5f}')\n",
        "    print(f'Loss on {name} data: {loss:.5f}')\n",
        "    for i,class_f1 in enumerate(f1s):\n",
        "      print(f'{mapping[i]} F1: {class_f1}')\n",
        "\n",
        "    if print_confusion_matrix:\n",
        "      print(''.join(['actual\\\\predicted'] + [str(label).rjust(12) for label in range(6)]))\n",
        "      for true_label in range(6):\n",
        "          print(''.join([str(true_label).rjust(16)] + [\n",
        "                  str(confusion_counts[true_label, pred_label]).rjust(12)\n",
        "                  for pred_label in range(6)]))\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, train_set, dev_set, class_weights, lr=1e-1, batch_size=32, num_epochs=30):\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Cross-entropy loss is just softmax regression loss\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    # Stochastic gradient descent optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Simple version of early stopping: save the best model checkpoint based on dev accuracy\n",
        "    best_dev_acc = -1\n",
        "    best_dev_f1 = -1\n",
        "    best_checkpoint_acc = None\n",
        "    best_checkpoint_f1 = None\n",
        "    best_epoch_acc = -1\n",
        "    best_epoch_f1 = -1\n",
        "    best_dev_loss = 1000\n",
        "    best_checkpoint_loss = None\n",
        "    best_epoch_loss = -1\n",
        "    # total_num_of_batches =  math.ceil(len(train_set)/batch_size)\n",
        "\n",
        "    epoch_train_f1 = []\n",
        "    epoch_dev_f1 = []\n",
        "    epoch_dev_loss = []\n",
        "\n",
        "\n",
        "    for t in range(num_epochs):\n",
        "        train_num_correct = 0\n",
        "\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "\n",
        "        # Training loop\n",
        "        model.train()  # Set model to \"training mode\", e.g. turns dropout on if you have dropout layers\n",
        "        # for batch in DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: preprocess_batch(batch, max_words)):\n",
        "        for batch in DataLoader(train_set, batch_size=batch_size, shuffle=True):\n",
        "            # DataLoader automatically groups the data into batchse of roughly batch_size\n",
        "            # shuffle=True makes it so that the batches are randomly chosen in each epoch\n",
        "            # unpack batch, which is a tuple (x_batch, y_batch)\n",
        "\n",
        "\n",
        "            x_batch, y_batch, seq_len_batch = batch\n",
        "            x_batch, y_batch, seq_len_batch = x_batch.to(device), y_batch.to(device), seq_len_batch\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()  # Reset the gradients to zero\n",
        "            # Recall how backpropagation works---gradients are initialized to zero and then accumulated\n",
        "            # So we need to reset to zero before running on a new batch!\n",
        "            # tensor of size (B, C), each row is the logits (pre-softmax scores) for the C classes\n",
        "            logits = model(x_batch, seq_len_batch)\n",
        "\n",
        "            # For MNIST, C=10\n",
        "            # Compute the loss of the model output compared to true labels\n",
        "            loss = loss_func(logits, y_batch)\n",
        "            loss.backward()  # Run backpropagation to compute gradients\n",
        "            optimizer.step()  # Take a SGD step\n",
        "            # Note that when we created the optimizer, we passed in model.parameters()\n",
        "            # This is a list of all parameters of all layers of the model\n",
        "            # optimizer.step() iterates over this list and does an SGD update to each parameter\n",
        "\n",
        "            # Compute running count of number of training examples correct\n",
        "            # Choose argmax for each row (i.e., collapse dimension 1, hence dim=1)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            train_num_correct += torch.sum(preds == y_batch).item()\n",
        "\n",
        "            y_pred += preds.tolist()\n",
        "            y_true += y_batch.tolist()\n",
        "\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        # Evaluate train and dev accuracy at the end of each epoch\n",
        "        train_acc = train_num_correct / len(train_set)\n",
        "\n",
        "\n",
        "        train_f1s = f1_score(y_true, y_pred, average=None)\n",
        "        train_f1 = np.dot(class_weights, np.array(train_f1s))\n",
        "        epoch_train_f1.append(train_f1.item())\n",
        "\n",
        "        # Set model to \"eval mode\", e.g. turns dropout off if you have dropout layers.\n",
        "        model.eval()\n",
        "        dev_loss=[]\n",
        "        with torch.no_grad():  # Don't allocate memory for storing gradients, more efficient when not training\n",
        "            for batch in DataLoader(dev_set, batch_size=len(dev_set), shuffle=True):\n",
        "                X_dev, y_dev, seq_len_batch = batch\n",
        "                X_dev, y_dev, seq_len_batch = X_dev.to(device), y_dev.to(device), seq_len_batch\n",
        "                dev_logits = model(X_dev, seq_len_batch)\n",
        "                dev_preds = torch.argmax(dev_logits, dim=1)\n",
        "\n",
        "                loss = loss_func(dev_logits, y_dev) #average loss\n",
        "\n",
        "                dev_acc = torch.mean((dev_preds == y_dev).float()).item()\n",
        "                dev_f1s = f1_score(y_dev.tolist(), dev_preds.tolist(), average=None)\n",
        "                dev_f1 = np.dot(class_weights, dev_f1s)\n",
        "\n",
        "\n",
        "                epoch_dev_f1.append(dev_f1.item())\n",
        "\n",
        "                if loss < best_dev_loss:\n",
        "                  best_dev_loss = loss\n",
        "                  best_checkpoint_loss = copy.deepcopy(model.state_dict())\n",
        "                  best_epoch_loss = t\n",
        "\n",
        "                if dev_acc > best_dev_acc:\n",
        "                    # Save this checkpoint if it has best dev accuracy so far\n",
        "                    best_dev_acc = dev_acc\n",
        "                    best_checkpoint_acc = copy.deepcopy(model.state_dict())\n",
        "                    best_epoch_acc = t\n",
        "\n",
        "                if dev_f1 > best_dev_f1:\n",
        "                  best_dev_f1 = dev_f1\n",
        "                  best_checkpoint_f1 = copy.deepcopy(model.state_dict())\n",
        "                  best_epoch_f1 = t\n",
        "\n",
        "\n",
        "        print(\n",
        "            f'Epoch {t: <2}: train_acc={train_acc:.5f}, dev_acc={dev_acc:.5f}, train_f1={train_f1:.5f}, dev_f1={dev_f1:.5f} dev_loss={loss:.5f}')\n",
        "\n",
        "    # Set the model parameters to the best checkpoint across all epochs\n",
        "    model.load_state_dict(best_checkpoint_f1)\n",
        "    end_time = time.time()\n",
        "    print(f'Training took {end_time - start_time:.2f} seconds')\n",
        "    print(f'\\nBest epoch in terms of accuracy was {best_epoch_acc}, dev_acc={best_dev_acc:.5f}')\n",
        "    print(f'Best epoch in terms of dev loss {best_epoch_loss}, dev_f1={best_dev_loss:.5f}')\n",
        "    print(f'Best epoch in terms of f1 was {best_epoch_f1}, dev_f1={best_dev_f1:.5f}\\n')\n",
        "\n",
        "    evaluate(model, dev_set, \"dev\", class_weights, print_confusion_matrix=True)\n",
        "    return epoch_train_f1, epoch_dev_f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def analyze_mistakes(dataset, model, vocabulary):\n",
        "\n",
        "    #  dataset = dataset[] #get a random split here\n",
        "     with torch.no_grad():  # Don't allocate memory for storing gradients, more efficient when not training\n",
        "      for batch in DataLoader(dataset, batch_size=len(dataset), shuffle=False):\n",
        "        X_test, y_test, seq_len_batch = batch\n",
        "        X_test, y_test, seq_len_batch = X_test.to(device), y_test.to(device), seq_len_batch\n",
        "        logits = model(X_test, seq_len_batch)  # tensor of size (N, 10)\n",
        "        # Choose argmax for each row (i.e., collapse dimension 1, hence dim=1)\n",
        "        y_preds = torch.argmax(logits, dim=1)\n",
        "        acc = torch.mean((y_preds == y_test).float()).item()\n",
        "        zipped = zip(y_test.tolist(), y_preds.tolist())\n",
        "        count = 0\n",
        "        for i, pair in enumerate(zipped):\n",
        "            if pair[0] == pair[1] or count > 10:\n",
        "                continue\n",
        "            words_list = vocabulary.lookup_tokens(X_test[i].tolist())\n",
        "            words = []\n",
        "            for word in words_list:\n",
        "                if word != \"<UNK>\":\n",
        "                    words.append(word)\n",
        "            sentence = \" \".join(words)\n",
        "            print(f'Text: {sentence}')\n",
        "            print(f'True label: {mapping[pair[0]]}')\n",
        "            print(f'Pred label: {mapping[pair[1]]}\\n')\n",
        "            count +=1\n",
        "\n",
        "def preprocess_data(X, y, vocab, name):\n",
        "    # map to indices\n",
        "    # padding\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    seq_len = []\n",
        "    counter = 0\n",
        "    for words, label in tqdm(zip(X, y), total=len(y), desc=f'Preprocessing {name} data'):\n",
        "        counter+=1\n",
        "        # if(counter % 50000 == 0):\n",
        "        #     print(f'Example {counter}/{len(y)}')\n",
        "        indices = vocab(words)\n",
        "        seq_len.append(len(indices))\n",
        "        X_list.append(torch.tensor(indices, dtype=torch.int32))\n",
        "        y_list.append(torch.tensor(label))\n",
        "\n",
        "    padded_X = pad_sequence(X_list, batch_first=True, padding_value = 0)\n",
        "\n",
        "    return padded_X, y_list, seq_len\n",
        "\n",
        "\n",
        "\n",
        "def build_vocabulary(dataset):\n",
        "\n",
        "    for text in dataset:\n",
        "        yield text\n",
        "\n",
        "\n",
        "def map_vocab_to_embeddings(vocabulary, word_vectors):\n",
        "    itos = vocabulary.get_itos()\n",
        "    vocab_size = len(vocabulary)\n",
        "    weights = torch.zeros((vocab_size, embed_size), dtype=torch.float32)\n",
        "    for i, word in enumerate(itos):\n",
        "        weights[i] = word_vectors[word]\n",
        "    return weights\n",
        "\n",
        "\n",
        "def get_class_weights(y,a):\n",
        "    c = Counter(y)\n",
        "    denom = sum([c[w]**a for w in range(6)])\n",
        "    # class_logits = [(c[w]/c.total())**a for w in range(6)]\n",
        "    class_weights = [((c[w]**a)/denom) for w in range(6)]\n",
        "    # class_weights = np.array([w/sum(class_logits) for w in class_logits])\n",
        "    return class_weights\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    #load data\n",
        "    X_train = read_file(X_train_filename)\n",
        "    X_dev = read_file(X_dev_filename)\n",
        "    X_test = read_file(X_test_filename)\n",
        "    y_train = np.load(y_train_filename)\n",
        "    y_dev = np.load(y_dev_filename)\n",
        "    y_test = np.load(y_test_filename)\n",
        "\n",
        "\n",
        "    class_weights = get_class_weights(y_train, a=0.25)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Creating Vocabulary from dataset...')\n",
        "    vocabulary = build_vocab_from_iterator(build_vocabulary(X_train + X_dev + X_test), min_freq=2, specials=[\"<UNK>\"])\n",
        "    vocabulary.set_default_index(vocabulary[\"<UNK>\"])\n",
        "\n",
        "    print(f'Reading GloVe...')\n",
        "    word_vectors = Vectors(name=word_vectors_filename)\n",
        "    print(f'Getting weights...')\n",
        "    weights = map_vocab_to_embeddings(vocabulary=vocabulary, word_vectors=word_vectors)\n",
        "\n",
        "\n",
        "    X_train, y_train, seq_len_train = preprocess_data(X_train, y_train, vocabulary, name=\"train\") #[ ([indices], label) ]\n",
        "    X_dev, y_dev, seq_len_dev = preprocess_data(X_dev, y_dev, vocabulary, name=\"dev\")\n",
        "    X_test, y_test, seq_len_test = preprocess_data(X_test, y_test, vocabulary, name=\"test\")\n",
        "    train_set = EmotionDataset(X_train, y_train, seq_len_train)\n",
        "    dev_set = EmotionDataset(X_dev, y_dev, seq_len_dev)\n",
        "    test_set = EmotionDataset(X_test, y_test, seq_len_test)\n",
        "\n",
        "    print(\"Training...\")\n",
        "\n",
        "\n",
        "    model = MyLSTM(vocab_size=len(vocabulary), weights=weights, hidden_size=hidden_dim, n_layers=n_layers, dropout = dropout, embed_size=embed_size).to(device)\n",
        "\n",
        "    train_accuracies, dev_accuracies = train(model, train_set, dev_set, lr=1e-1,\n",
        "          batch_size=batch_size, num_epochs=epochs, class_weights=class_weights)\n",
        "\n",
        "\n",
        "    evaluate(model, test_set, \"test\", class_weights, print_confusion_matrix=True)\n",
        "    analyze_mistakes(dev_set, model, vocabulary)\n",
        "\n",
        "    # Create a line plot of accuracy over epochs\n",
        "    plt.plot(range(1, epochs+1), train_accuracies, linestyle='-', label=\"Train\")\n",
        "    plt.plot(range(1, epochs+1), dev_accuracies, linestyle='-', label=\"Dev\")\n",
        "    plt.title('Weighted F1 per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Weighted F1')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    txt=f'Embedding Size: {embed_size} - Hidden Size: {hidden_dim} - Batch Size: {batch_size} - Num Layers: {n_layers}\\n Dropout = {dropout}'\n",
        "    plt.figtext(0.5, -0.035, txt, wrap=True, horizontalalignment='center', fontsize=9)\n",
        "\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}